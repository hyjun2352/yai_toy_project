{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6cc7f2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Downloading tensorflow_intel-2.11.0-cp39-cp39-win_amd64.whl (266.3 MB)\n",
      "     -------------------------------------- 266.3/266.3 MB 3.6 MB/s eta 0:00:00\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.51.3-cp39-cp39-win_amd64.whl (3.7 MB)\n",
      "     ---------------------------------------- 3.7/3.7 MB 10.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\khyj0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (63.4.1)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 11.9 MB/s eta 0:00:00\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\khyj0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 10.5 MB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "     -------------------------------------- 439.2/439.2 kB 6.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\khyj0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-15.0.6.1-py2.py3-none-win_amd64.whl (23.2 MB)\n",
      "     --------------------------------------- 23.2/23.2 MB 10.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\khyj0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\khyj0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\khyj0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "     ---------------------------------------- 6.0/6.0 MB 11.6 MB/s eta 0:00:00\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-win_amd64.whl (895 kB)\n",
      "     ------------------------------------- 895.9/895.9 kB 11.4 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\khyj0\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.3.0)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     -------------------------------------- 126.5/126.5 kB 7.3 MB/s eta 0:00:00\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\khyj0\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\khyj0\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.16.1-py2.py3-none-any.whl (177 kB)\n",
      "     ---------------------------------------- 177.2/177.2 kB ? eta 0:00:00\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     ------------------------------------- 781.3/781.3 kB 12.4 MB/s eta 0:00:00\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\khyj0\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\khyj0\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\khyj0\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\khyj0\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\khyj0\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\khyj0\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\khyj0\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\khyj0\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\khyj0\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 flatbuffers-23.1.21 gast-0.4.0 google-auth-2.16.1 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.3 keras-2.11.0 libclang-15.0.6.1 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.2.0\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a5e8865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import MySQLdb\n",
    "import sklearn\n",
    "\n",
    "# import konlpy\n",
    "# from konlpy.tag import Twitter\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim import corpora, models\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim\n",
    "import pickle\n",
    "\n",
    "# from wordcloud import WordCloud\n",
    "# from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "from os import path\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, value\n",
    "# output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7abe1598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>제목</th>\n",
       "      <th>가사</th>\n",
       "      <th>가수</th>\n",
       "      <th>발매일</th>\n",
       "      <th>좋아요수</th>\n",
       "      <th>장르</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>이제 나만 믿어요</td>\n",
       "      <td>무얼 믿은 걸까 부족했던 내게서\\n나조차 못 믿던 내게 여태 머문 사람\\n무얼 봤던...</td>\n",
       "      <td>임영웅</td>\n",
       "      <td>2020.04.03</td>\n",
       "      <td>164,747</td>\n",
       "      <td>성인가요/트로트</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>보금자리</td>\n",
       "      <td>그대 사랑이 나였음 좋겠다\\n아무것도 필요 없어요\\n든든한 품에 안겨 잠들고 싶어라...</td>\n",
       "      <td>임영웅</td>\n",
       "      <td>2022.05.02</td>\n",
       "      <td>43,816</td>\n",
       "      <td>성인가요/트로트</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>찐이야</td>\n",
       "      <td>찐찐찐찐 찐이야 완전 찐이야\\n진짜가 나타났다 지금\\n찐찐찐찐 찐이야 완전 찐이야\\...</td>\n",
       "      <td>영탁</td>\n",
       "      <td>2020.03.13</td>\n",
       "      <td>66,514</td>\n",
       "      <td>성인가요/트로트</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>니가 왜 거기서 나와 (Narr. 고은아)</td>\n",
       "      <td>어디야\\n집이야 피곤해서 일찍 자려구\\n아 그래 잠깐 볼랬더니\\n오늘 피곤했나 보네...</td>\n",
       "      <td>영탁</td>\n",
       "      <td>2018.10.21</td>\n",
       "      <td>61,103</td>\n",
       "      <td>성인가요/트로트</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>별빛 같은 나의 사랑아</td>\n",
       "      <td>당신이 얼마나 내게\\n소중한 사람인지\\n세월이 흐르고 보니\\n이제 알 것 같아요\\n...</td>\n",
       "      <td>임영웅</td>\n",
       "      <td>2021.03.09</td>\n",
       "      <td>76,133</td>\n",
       "      <td>성인가요/트로트</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        제목                                                 가사  \\\n",
       "0                이제 나만 믿어요  무얼 믿은 걸까 부족했던 내게서\\n나조차 못 믿던 내게 여태 머문 사람\\n무얼 봤던...   \n",
       "1                     보금자리  그대 사랑이 나였음 좋겠다\\n아무것도 필요 없어요\\n든든한 품에 안겨 잠들고 싶어라...   \n",
       "2                      찐이야  찐찐찐찐 찐이야 완전 찐이야\\n진짜가 나타났다 지금\\n찐찐찐찐 찐이야 완전 찐이야\\...   \n",
       "3  니가 왜 거기서 나와 (Narr. 고은아)  어디야\\n집이야 피곤해서 일찍 자려구\\n아 그래 잠깐 볼랬더니\\n오늘 피곤했나 보네...   \n",
       "4             별빛 같은 나의 사랑아  당신이 얼마나 내게\\n소중한 사람인지\\n세월이 흐르고 보니\\n이제 알 것 같아요\\n...   \n",
       "\n",
       "    가수         발매일     좋아요수        장르  \n",
       "0  임영웅  2020.04.03  164,747  성인가요/트로트  \n",
       "1  임영웅  2022.05.02   43,816  성인가요/트로트  \n",
       "2   영탁  2020.03.13   66,514  성인가요/트로트  \n",
       "3   영탁  2018.10.21   61,103  성인가요/트로트  \n",
       "4  임영웅  2021.03.09   76,133  성인가요/트로트  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiphop = pd.read_csv('melon_trot_1_3500.csv')\n",
    "hiphop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75477f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['제목', '가사', '가수', '발매일', '좋아요수', '장르'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiphop.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a9f3851",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PREMADE_TEXT = False\n",
    "\n",
    "text_filepath = 'word2vec/all_lyrics_text.txt'\n",
    "if not USE_PREMADE_TEXT:\n",
    "    with open(text_filepath, 'w', encoding='utf-8') as f:\n",
    "        for lyrics in hiphop['가사'].values:\n",
    "            if pd.isnull(lyrics): # null값 있다면 그 다음으로 넘어감\n",
    "                continue\n",
    "            f.write(lyrics + '\\n')\n",
    "else:\n",
    "    assert path.exists(text_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2359ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PREMADE_BIGRAM_MODEL = False\n",
    "\n",
    "all_bigram_model_filepath = 'word2vec/all_bigram_model'\n",
    "all_sentences_normalized_filepath = 'word2vec/all_lyrics_text.txt'\n",
    "\n",
    "all_unigram_sentences = LineSentence(all_sentences_normalized_filepath)\n",
    "\n",
    "if not USE_PREMADE_BIGRAM_MODEL:    \n",
    "    \n",
    "    all_bigram_model = Phrases(all_unigram_sentences) #phrase냐 아니냐를 판단해줌\n",
    "    all_bigram_model.save(all_bigram_model_filepath)\n",
    "    \n",
    "else:\n",
    "    all_bigram_model = Phrases.load(all_bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5630ba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PREMADE_BIGRAM_SENTENCES = False\n",
    "\n",
    "all_bigram_sentences_filepath = 'word2vec/all_sentences_for_word2vec.txt'\n",
    "\n",
    "if not USE_PREMADE_BIGRAM_SENTENCES:\n",
    "    \n",
    "    with open(all_bigram_sentences_filepath, 'w', encoding='utf-8') as f:\n",
    "        for unigram_sentence in all_unigram_sentences:\n",
    "            all_bigram_sentence = all_bigram_model[unigram_sentence]\n",
    "            f.write(' '.join(all_bigram_sentence) + '\\n')\n",
    "else:\n",
    "    assert path.exists(all_bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45937851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khyj0\\AppData\\Local\\Temp\\ipykernel_18228\\1412957348.py:21: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  all2vec.init_sims()\n"
     ]
    }
   ],
   "source": [
    "USE_PREMADE_WORD2VEC = False\n",
    "\n",
    "all2vec_filepath = 'word2vec/all_word2vec_model'\n",
    "\n",
    "if not USE_PREMADE_WORD2VEC:\n",
    "    \n",
    "    lyrics_for_word2vec = LineSentence(all_bigram_sentences_filepath)\n",
    "\n",
    "    all2vec = Word2Vec(lyrics_for_word2vec, vector_size=100, window=5, min_count=1, sg=1)\n",
    "    # sg=0 cbow 1=Skip-Gram Model\n",
    "    # 100차원으로 가져옴 / 보통 20~100 정도\n",
    "    # window = 5 앞 5개, 뒤 5개 단어를 보겠다는 뜻\n",
    "    # window size 작을수록 문법적인 의미가 너무 중요해짐, 클수록 주제 지향적으로 문맥적인 정보를 많이 담게 됨    \n",
    "    for _ in range(9):\n",
    "        all2vec.train(lyrics_for_word2vec, total_examples=81085, epochs=1)\n",
    "        # 9139곡 가사에 총 789847 문장이 있다는 것 명시\n",
    "        \n",
    "    all2vec.save(all2vec_filepath)\n",
    "else:\n",
    "    all2vec = Word2Vec.load(all2vec_filepath)\n",
    "all2vec.init_sims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba2c9f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all2vec_filepath = 'word2vec/all_word2vec_model'\n",
    "all2vec = Word2Vec.load(all2vec_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c43958f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('얘기_들려줬지', 0.8526408672332764),\n",
       " ('어릿광대의_서글픈', 0.8409197926521301),\n",
       " ('허락한', 0.8371437191963196),\n",
       " ('지워지지않는', 0.8337648510932922),\n",
       " ('사랑하시렵니까', 0.8337077498435974)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all2vec.wv.most_similar(positive=['사랑'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d648f1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('눈매에', 0.8991658687591553),\n",
       " ('나는야_당신의', 0.8922415971755981),\n",
       " ('눈물인가', 0.8897567987442017),\n",
       " ('친정엄마', 0.8883345723152161),\n",
       " ('컷이라도', 0.8877487182617188)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all2vec.wv.most_similar(positive=['엄마'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2780a58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>내</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>사랑</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>내가</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>나는</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0\n",
       "0   내\n",
       "1   그\n",
       "2  사랑\n",
       "3  내가\n",
       "4  나는"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame(all2vec.wv.index_to_key)\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c61b7464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32851"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1b370fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3650"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "cnt = all2vec.wv.index_to_key[0]\n",
    "all2vec.wv.get_vecattr(cnt, \"count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a32ab550",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m a, b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28menumerate\u001b[39m(all2vec\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mkey_to_index)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "a, b = enumerate(all2vec.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "205e8ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 96.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words = []\n",
    "for i in (range(32851)):\n",
    "    cnt_num = all2vec.wv.index_to_key[i]\n",
    "    cnt = all2vec.wv.get_vecattr(cnt_num, \"count\")\n",
    "    if cnt > 100:\n",
    "        words.append(a[0][i]) # 횟수 100회 초과하는 단어만 리스트화\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c65c5e08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fb2bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in words:\n",
    "    X.append(all2vec.wv[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dedf630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acb5555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = pd.DataFrame(X, index = words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "feec5553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>내</th>\n",
       "      <td>-0.136166</td>\n",
       "      <td>0.366624</td>\n",
       "      <td>1.034507</td>\n",
       "      <td>1.433129</td>\n",
       "      <td>-0.514249</td>\n",
       "      <td>-0.298968</td>\n",
       "      <td>0.879357</td>\n",
       "      <td>1.351689</td>\n",
       "      <td>-1.294456</td>\n",
       "      <td>-0.493064</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.481747</td>\n",
       "      <td>-0.471407</td>\n",
       "      <td>-0.028197</td>\n",
       "      <td>0.012252</td>\n",
       "      <td>1.081335</td>\n",
       "      <td>0.171065</td>\n",
       "      <td>-1.172329</td>\n",
       "      <td>-1.360425</td>\n",
       "      <td>-0.586285</td>\n",
       "      <td>0.998504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>그</th>\n",
       "      <td>0.208582</td>\n",
       "      <td>0.517061</td>\n",
       "      <td>0.819431</td>\n",
       "      <td>0.818019</td>\n",
       "      <td>0.654620</td>\n",
       "      <td>-0.805044</td>\n",
       "      <td>0.646501</td>\n",
       "      <td>1.413978</td>\n",
       "      <td>-1.302137</td>\n",
       "      <td>-0.306315</td>\n",
       "      <td>...</td>\n",
       "      <td>0.320788</td>\n",
       "      <td>-0.193923</td>\n",
       "      <td>0.136352</td>\n",
       "      <td>-0.298862</td>\n",
       "      <td>0.322760</td>\n",
       "      <td>0.041108</td>\n",
       "      <td>0.493559</td>\n",
       "      <td>-0.702168</td>\n",
       "      <td>-0.784199</td>\n",
       "      <td>0.078243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>사랑</th>\n",
       "      <td>-0.367977</td>\n",
       "      <td>0.569756</td>\n",
       "      <td>-0.145142</td>\n",
       "      <td>-0.781577</td>\n",
       "      <td>0.019094</td>\n",
       "      <td>0.153369</td>\n",
       "      <td>-0.215093</td>\n",
       "      <td>1.892271</td>\n",
       "      <td>-0.320322</td>\n",
       "      <td>0.681078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063266</td>\n",
       "      <td>0.140259</td>\n",
       "      <td>0.286728</td>\n",
       "      <td>-0.030319</td>\n",
       "      <td>0.443175</td>\n",
       "      <td>0.322728</td>\n",
       "      <td>0.007324</td>\n",
       "      <td>-0.874286</td>\n",
       "      <td>0.623615</td>\n",
       "      <td>0.070719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>내가</th>\n",
       "      <td>-1.360877</td>\n",
       "      <td>1.106440</td>\n",
       "      <td>-0.188100</td>\n",
       "      <td>0.168360</td>\n",
       "      <td>0.812520</td>\n",
       "      <td>-0.721575</td>\n",
       "      <td>0.284537</td>\n",
       "      <td>1.743519</td>\n",
       "      <td>-0.041589</td>\n",
       "      <td>-0.961685</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022407</td>\n",
       "      <td>-0.083306</td>\n",
       "      <td>0.013059</td>\n",
       "      <td>-0.332200</td>\n",
       "      <td>1.154214</td>\n",
       "      <td>-0.729024</td>\n",
       "      <td>0.516737</td>\n",
       "      <td>-0.481769</td>\n",
       "      <td>-0.576209</td>\n",
       "      <td>-0.134269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>나는</th>\n",
       "      <td>-0.335103</td>\n",
       "      <td>-0.325015</td>\n",
       "      <td>-0.978000</td>\n",
       "      <td>0.705873</td>\n",
       "      <td>0.295354</td>\n",
       "      <td>-0.351638</td>\n",
       "      <td>0.327134</td>\n",
       "      <td>2.089930</td>\n",
       "      <td>-0.537545</td>\n",
       "      <td>0.199769</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736009</td>\n",
       "      <td>-0.377910</td>\n",
       "      <td>1.495826</td>\n",
       "      <td>0.605818</td>\n",
       "      <td>1.066280</td>\n",
       "      <td>-0.900749</td>\n",
       "      <td>0.506734</td>\n",
       "      <td>0.673915</td>\n",
       "      <td>0.262671</td>\n",
       "      <td>0.564055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "내  -0.136166  0.366624  1.034507  1.433129 -0.514249 -0.298968  0.879357   \n",
       "그   0.208582  0.517061  0.819431  0.818019  0.654620 -0.805044  0.646501   \n",
       "사랑 -0.367977  0.569756 -0.145142 -0.781577  0.019094  0.153369 -0.215093   \n",
       "내가 -1.360877  1.106440 -0.188100  0.168360  0.812520 -0.721575  0.284537   \n",
       "나는 -0.335103 -0.325015 -0.978000  0.705873  0.295354 -0.351638  0.327134   \n",
       "\n",
       "          7         8         9   ...        90        91        92        93  \\\n",
       "내   1.351689 -1.294456 -0.493064  ... -0.481747 -0.471407 -0.028197  0.012252   \n",
       "그   1.413978 -1.302137 -0.306315  ...  0.320788 -0.193923  0.136352 -0.298862   \n",
       "사랑  1.892271 -0.320322  0.681078  ...  0.063266  0.140259  0.286728 -0.030319   \n",
       "내가  1.743519 -0.041589 -0.961685  ... -0.022407 -0.083306  0.013059 -0.332200   \n",
       "나는  2.089930 -0.537545  0.199769  ...  0.736009 -0.377910  1.495826  0.605818   \n",
       "\n",
       "          94        95        96        97        98        99  \n",
       "내   1.081335  0.171065 -1.172329 -1.360425 -0.586285  0.998504  \n",
       "그   0.322760  0.041108  0.493559 -0.702168 -0.784199  0.078243  \n",
       "사랑  0.443175  0.322728  0.007324 -0.874286  0.623615  0.070719  \n",
       "내가  1.154214 -0.729024  0.516737 -0.481769 -0.576209 -0.134269  \n",
       "나는  1.066280 -0.900749  0.506734  0.673915  0.262671  0.564055  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "275b0add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khyj0\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:780: FutureWarning: The default initialization in TSNE will change from 'random' to 'pca' in 1.2.\n",
      "  warnings.warn(\n",
      "C:\\Users\\khyj0\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "USE_PREMADE_TSNE = False\n",
    "\n",
    "tsne_filepath = 'word2vec/tsne.pkl'\n",
    "\n",
    "if not USE_PREMADE_TSNE:\n",
    "    \n",
    "    tsne = TSNE(random_state=0)\n",
    "    tsne_points = tsne.fit_transform(X2)\n",
    "    with open(tsne_filepath, 'wb') as f:\n",
    "        pickle.dump(tsne_points, f)\n",
    "else:\n",
    "    with open(tsne_filepath, 'rb') as f:\n",
    "        tsne_points = pickle.load(f)\n",
    "\n",
    "tsne_df = pd.DataFrame(tsne_points, index=X2.index, columns=['x_coord', 'y_coord'])\n",
    "tsne_df['word'] = tsne_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "437c9900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data in a form suitable for bokeh.\n",
    "plot_data = ColumnDataSource(tsne_df)\n",
    "\n",
    "# create the plot and configure it\n",
    "tsne_plot = figure(title=\"t-SNE Word Embeddings\",\n",
    "                   plot_width = 800,\n",
    "                   plot_height = 800,\n",
    "                   active_scroll='wheel_zoom'\n",
    "                  )\n",
    "\n",
    "# add a hover tool to display words on roll-over\n",
    "tsne_plot.add_tools( HoverTool(tooltips = '@word') )\n",
    "\n",
    "tsne_plot.circle('x_coord', 'y_coord', source=plot_data,\n",
    "                 color='red', line_alpha=0.2, fill_alpha=0.1,\n",
    "                 size=10, hover_line_color='orange')\n",
    "\n",
    "# adjust visual elements of the plot\n",
    "tsne_plot.xaxis.visible = False\n",
    "tsne_plot.yaxis.visible = False\n",
    "tsne_plot.grid.grid_line_color = None\n",
    "tsne_plot.outline_line_color = None\n",
    "\n",
    "# show time!\n",
    "show(tsne_plot);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "384d822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "class TextLoader:\n",
    "    def __init__(self, path):\n",
    "        with open(path, \"r\", encoding='utf-8') as _file:\n",
    "            self.text = _file.read().split()\n",
    "            # self.text = self.text[:10000]\n",
    "\n",
    "        self.song2vec = Word2Vec.load(\"word2vec/all_word2vec_model\")\n",
    "        self.vocab, self.words = self.build_vocab()\n",
    "\n",
    "        self.X = self.text[:] # 텍스트 파일 전체를 복사 - self.text 와 같은 의미\n",
    "        self.y = [self.text[0]] + self.text[1:] # \n",
    "\n",
    "    def build_vocab(self):\n",
    "        vocab_inv = list(self.song2vec.wv.index_to_key) # key 값을 리스트화 / 글자만\n",
    "        vocab = {x: i for i, x in enumerate(vocab_inv)} # 0,1,2 등 인덱스와 단어를 dict 로 매칭시켜놓음\n",
    "        return vocab, vocab_inv\n",
    "\n",
    "    def next_batch(self, batch_size, seq_length):\n",
    "        start = np.random.randint(0, len(self.X)-batch_size*seq_length) # 랜덤으로 위치를 정함 - 끝의 값을 구하면 안됨 / 시작 위치를 글자를 다 배치사이즈와 시퀀스렝스로 구함 // 마지막까지는 안가겠다는 뜻\n",
    "        end   = start + batch_size*seq_length # 몇 단어를 가져올지\n",
    "\n",
    "        X_words = self.X[start:end]# 말그대로 글자\n",
    "        y_words = self.y[start:end]\n",
    "\n",
    "        X_idx = np.empty((batch_size, seq_length), dtype=np.int64) # 글자의 인덱스\n",
    "        y_idx = np.empty((batch_size, seq_length), dtype=np.int64)\n",
    "        X_wv = np.empty((batch_size, seq_length, 100)) # 글자의 word2vec \n",
    "        y_wv = np.empty((batch_size, seq_length, 100))\n",
    "        # 위에서 만들어준 자리에 따라 (저장공간 설정하는 과정) 아래에서 for 문을 돌며 값을 가져옴 / 그냥 하면 안되는 이유 : append는 느림, numpy의 경우에는 1 2 3 4 붙어있어야 함 / 5를 넣는다 하면 이걸 어딘가 복사해서 5를 붙여야 함.\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_length):\n",
    "                X_idx[i, j] = self.vocab[X_words[i*seq_length+j]]\n",
    "                y_idx[i, j] = self.vocab[y_words[i*seq_length+j]]\n",
    "\n",
    "                X_wv[i, j] = self.song2vec.wv[X_words[i*seq_length+j]]\n",
    "                y_wv[i, j] = self.song2vec.wv[y_words[i*seq_length+j]]\n",
    "\n",
    "        return X_wv, X_idx, y_wv, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff1a1d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "# import tensorflow.contrib.rnn as rnn\n",
    "# import tensorflow.contrib.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4cd0343",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers  = 3\n",
    "hidden_size = 512\n",
    "batch_size  = 1 # 1글자\n",
    "# max_length  = 1 # 1로 하는것으로~\n",
    "\n",
    "loader = TextLoader(\"word2vec/all_sentences_for_word2vec.txt\")\n",
    "vocab_size = len(loader.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8470889a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(hidden_size, num_layers, vocab_size)\n\u001b[1;32m---> 24\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[44], line 18\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     17\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 18\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[0;32m     20\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:765\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    762\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    763\u001b[0m             msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor unbatched 2-D input, hx and cx should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    764\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malso be 2-D but got (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D) tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 765\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[0;32m    766\u001b[0m         hx \u001b[38;5;241m=\u001b[39m (hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    768\u001b[0m \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx and cx should also be 2-D but got (3-D, 3-D) tensors"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X = torch.zeros((batch_size, vocab_size)) # assuming you have already loaded your data and created the tensors\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, vocab_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.lstm = nn.LSTM(input_size=vocab_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out.reshape(-1, self.hidden_size)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = Model(hidden_size, num_layers, vocab_size)\n",
    "outputs = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c49b45a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕', '나는', '너야']\n",
      "Start with: 안녕 나는 너야\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v2.train' has no attribute 'Saver'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentence)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart with:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sentence))\n\u001b[1;32m----> 6\u001b[0m saver \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSaver\u001b[49m()\n\u001b[0;32m      7\u001b[0m sess_config \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mConfigProto(gpu_options\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mGPUOptions(allow_growth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mSession(config\u001b[38;5;241m=\u001b[39msess_config) \u001b[38;5;28;01mas\u001b[39;00m sess:\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.train' has no attribute 'Saver'"
     ]
    }
   ],
   "source": [
    "# 시작 글자 생성\n",
    "sentence = [\"안녕\", \"나는\", \"너야\"]\n",
    "print(sentence)\n",
    "print(\"Start with:\", \" \".join(sentence))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver.restore(sess, \"char-rnn_230000\")\n",
    "    \n",
    "    # [배치사이즈, max_length, 100]\n",
    "    vec = np.empty((1, len(sentence), 100)) # 시작 글자를 주고 다음 단어를 예측 - 시작으로 준 것을 전부 다 입력으로 넣겠다\n",
    "    for i, word in enumerate(sentence):\n",
    "        vec[:, i, :] = loader.song2vec.wv[word]\n",
    "    \n",
    "    # 매 이터레이션마다 글자 하나씩 생성\n",
    "    state = sess.run(states, feed_dict={X: vec}) #입력단어 sentence 이후에 들어올 단어를 예측\n",
    "    for i in range(15): # for문을 돌면서 풀어헤치는 중!!\n",
    "        vec = loader.song2vec.wv[sentence[-1]].reshape(1, 1, 100)\n",
    "        \n",
    "        pred_char, state = sess.run([pred, states], \n",
    "            feed_dict={X: vec, initial_state: state}) # 원래 initial stete 는 0 이었으나\n",
    "        # 이전 스텝에 갖고 있는 state 값을 다음 스텝에 넣어줌 // 입력단어들을 그 다음 스텝에 넣어주는 것임\n",
    "        \n",
    "        pred_char = loader.words[pred_char[0][-1]]\n",
    "        sentence.append(pred_char)\n",
    "\n",
    "for i, word in enumerate(sentence):\n",
    "    print(word, end=\" \")\n",
    "    if (i+1) % 5 == 0:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ae445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
